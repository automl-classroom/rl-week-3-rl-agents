{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37d9151-dcbf-4c7f-8d0b-c697b3fdbc13",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RL Exercise Demo\n",
    "This exercise serves as a demonstration how to quickly train an RL agent on a popular environment with a RL framework.\n",
    "We will\n",
    "1. Select and instantiate environment (gym's BipedalWalker-v3).\n",
    "2. Select and setup our RL algorithm / agent (stablebaselines3 [SAC](https://spinningup.openai.com/en/latest/algorithms/sac.html)).\n",
    "3. Train the agent on the environment and visualize training progress.\n",
    "4. Evaluate our agent and observe the distribution of test performances.\n",
    "5. Record and replay the agent, before and after."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c4b5e2-e487-4b04-8c55-3938b4d82529",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Environment: Bipedal Walker\n",
    "![Bipedal Walker](bipedal_walker.gif) [credits](https://www.gymlibrary.dev/_images/bipedal_walker.gif)\n",
    "\n",
    "![Interaction Env-Agent](env_agent.png)\n",
    "\n",
    "### Env\n",
    "- locomotion\n",
    "- 4 joints\n",
    "\n",
    "### Action Space\n",
    "- motor speed for all 4 joints (hips and knees) [-1, 1]\n",
    "\n",
    "### Observation Space\n",
    "State consists of\n",
    "- hull angle speed\n",
    "- angular velocity\n",
    "- horizontal speed\n",
    "- vertical speed\n",
    "- position of joints\n",
    "- joints angular speed\n",
    "- legs contact with ground\n",
    "- 10 lidar rangefinder measurements\n",
    "\n",
    "There are no coordinates! State vector with 24 entries.\n",
    "\n",
    "### Rewards\n",
    "Moving forward gives rewards.\n",
    "Falling is punished.\n",
    "Applying motor torque costs a little.\n",
    "\n",
    "\n",
    "### Starting State\n",
    "Stands at the left in a certain position.\n",
    "\n",
    "### Episode Termination\n",
    "- walker falls\n",
    "- or reaches end of terrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22de499d-f2cd-4d95-b61b-1827650d001b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_fn = \"trained_agent.zip\"\n",
    "env_id = \"BipedalWalker-v3\"\n",
    "log_dir = \"logs/tensorboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267d6a0-dcd5-42d2-83e2-a70cf4518e36",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Agent: SAC\n",
    "SAC: Soft Actor Critic\n",
    "[[paper]](https://arxiv.org/abs/1801.01290) [[blogpost]](https://spinningup.openai.com/en/latest/algorithms/sac.html)\n",
    "\n",
    "- off-policy algorithm\n",
    "- for continuous actions\n",
    "- one actor, one critic\n",
    "- special regularization to steer exploration-exploitation trade-off "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b794a7c-86f1-4675-aa8f-9e81f2ca4f4b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to logs/tensorboard/SAC_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 833      |\n",
      "|    ep_rew_mean     | -95.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 3332     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.112    |\n",
      "|    ent_coef        | 0.38     |\n",
      "|    ent_coef_loss   | -6.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3231     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 639      |\n",
      "|    ep_rew_mean     | -98.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 5111     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -27.9    |\n",
      "|    critic_loss     | 0.67     |\n",
      "|    ent_coef        | 0.226    |\n",
      "|    ent_coef_loss   | -8.89    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5010     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 572      |\n",
      "|    ep_rew_mean     | -99.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 99       |\n",
      "|    total_timesteps | 6860     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -27.9    |\n",
      "|    critic_loss     | 1.88     |\n",
      "|    ent_coef        | 0.137    |\n",
      "|    ent_coef_loss   | -9.94    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6759     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 450      |\n",
      "|    ep_rew_mean     | -101     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 104      |\n",
      "|    total_timesteps | 7197     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -27.5    |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    ent_coef        | 0.125    |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7096     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 451      |\n",
      "|    ep_rew_mean     | -101     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 128      |\n",
      "|    total_timesteps | 9019     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26      |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0757   |\n",
      "|    ent_coef_loss   | -12.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8918     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 453      |\n",
      "|    ep_rew_mean     | -103     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 155      |\n",
      "|    total_timesteps | 10866    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 12.7     |\n",
      "|    ent_coef        | 0.0463   |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10765    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 415      |\n",
      "|    ep_rew_mean     | -107     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 167      |\n",
      "|    total_timesteps | 11619    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.7    |\n",
      "|    critic_loss     | 6.08     |\n",
      "|    ent_coef        | 0.039    |\n",
      "|    ent_coef_loss   | -7.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11518    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 376      |\n",
      "|    ep_rew_mean     | -108     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 173      |\n",
      "|    total_timesteps | 12031    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.691    |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | -10      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11930    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 350      |\n",
      "|    ep_rew_mean     | -109     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 180      |\n",
      "|    total_timesteps | 12618    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.3    |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.0305   |\n",
      "|    ent_coef_loss   | -8.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12517    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 326      |\n",
      "|    ep_rew_mean     | -108     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 186      |\n",
      "|    total_timesteps | 13043    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.8    |\n",
      "|    critic_loss     | 45.1     |\n",
      "|    ent_coef        | 0.0276   |\n",
      "|    ent_coef_loss   | -7.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12942    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 305      |\n",
      "|    ep_rew_mean     | -108     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 191      |\n",
      "|    total_timesteps | 13433    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.8    |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | -5.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13332    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 286      |\n",
      "|    ep_rew_mean     | -109     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 195      |\n",
      "|    total_timesteps | 13742    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.6    |\n",
      "|    critic_loss     | 39.9     |\n",
      "|    ent_coef        | 0.0238   |\n",
      "|    ent_coef_loss   | -8.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13641    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 273      |\n",
      "|    ep_rew_mean     | -109     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 14197    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16      |\n",
      "|    critic_loss     | 1.54     |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -2.65    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14096    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 261      |\n",
      "|    ep_rew_mean     | -109     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 209      |\n",
      "|    total_timesteps | 14626    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.5    |\n",
      "|    critic_loss     | 2.18     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | -3.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14525    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 250      |\n",
      "|    ep_rew_mean     | -109     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 14975    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17      |\n",
      "|    critic_loss     | 2.19     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | -2.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14874    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 243      |\n",
      "|    ep_rew_mean     | -110     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 221      |\n",
      "|    total_timesteps | 15535    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.8    |\n",
      "|    critic_loss     | 3.86     |\n",
      "|    ent_coef        | 0.0185   |\n",
      "|    ent_coef_loss   | -0.898   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15434    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 234      |\n",
      "|    ep_rew_mean     | -109     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 226      |\n",
      "|    total_timesteps | 15919    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.2    |\n",
      "|    critic_loss     | 2.49     |\n",
      "|    ent_coef        | 0.0187   |\n",
      "|    ent_coef_loss   | -4.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15818    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 230      |\n",
      "|    ep_rew_mean     | -110     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 237      |\n",
      "|    total_timesteps | 16579    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.5    |\n",
      "|    critic_loss     | 4.11     |\n",
      "|    ent_coef        | 0.0193   |\n",
      "|    ent_coef_loss   | 2.84     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16478    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 225      |\n",
      "|    ep_rew_mean     | -111     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 244      |\n",
      "|    total_timesteps | 17086    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.9    |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -0.705   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16985    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 223      |\n",
      "|    ep_rew_mean     | -112     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 254      |\n",
      "|    total_timesteps | 17860    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.3    |\n",
      "|    critic_loss     | 2.84     |\n",
      "|    ent_coef        | 0.0216   |\n",
      "|    ent_coef_loss   | -0.742   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 17759    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 216      |\n",
      "|    ep_rew_mean     | -112     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 258      |\n",
      "|    total_timesteps | 18131    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.5    |\n",
      "|    critic_loss     | 2.75     |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | 1.25     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18030    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 236      |\n",
      "|    ep_rew_mean     | -113     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 295      |\n",
      "|    total_timesteps | 20753    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.65    |\n",
      "|    critic_loss     | 1.95     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | 1.25     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20652    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 295      |\n",
      "|    ep_rew_mean     | -113     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 386      |\n",
      "|    total_timesteps | 27153    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.9    |\n",
      "|    critic_loss     | 47.1     |\n",
      "|    ent_coef        | 0.022    |\n",
      "|    ent_coef_loss   | 0.908    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 27052    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 319      |\n",
      "|    ep_rew_mean     | -112     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 434      |\n",
      "|    total_timesteps | 30580    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.44    |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | -0.581   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 30479    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 340      |\n",
      "|    ep_rew_mean     | -111     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 483      |\n",
      "|    total_timesteps | 34033    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.44    |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0166   |\n",
      "|    ent_coef_loss   | -0.448   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33932    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 329      |\n",
      "|    ep_rew_mean     | -110     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 514      |\n",
      "|    total_timesteps | 36205    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.53    |\n",
      "|    critic_loss     | 0.799    |\n",
      "|    ent_coef        | 0.0162   |\n",
      "|    ent_coef_loss   | -1.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 36104    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 319      |\n",
      "|    ep_rew_mean     | -111     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 525      |\n",
      "|    total_timesteps | 37017    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.36    |\n",
      "|    critic_loss     | 0.993    |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | 2.44     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 36916    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 351      |\n",
      "|    ep_rew_mean     | -109     |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 594      |\n",
      "|    total_timesteps | 41926    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.52    |\n",
      "|    critic_loss     | 0.868    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -1.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 41825    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m env = gym.make(env_id)\n\u001b[32m      5\u001b[39m model = SAC(\u001b[33m\"\u001b[39m\u001b[33mMlpPolicy\u001b[39m\u001b[33m\"\u001b[39m, env, verbose=\u001b[32m1\u001b[39m, tensorboard_log=log_dir)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m model.save(model_fn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RL/exercises/rl-week-3-rl-agents/.venv/lib/python3.11/site-packages/stable_baselines3/sac/sac.py:308\u001b[39m, in \u001b[36mSAC.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    300\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[32m    301\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    306\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    307\u001b[39m ) -> SelfSAC:\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RL/exercises/rl-week-3-rl-agents/.venv/lib/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:347\u001b[39m, in \u001b[36mOffPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    345\u001b[39m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[32m    346\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m callback.on_training_end()\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RL/exercises/rl-week-3-rl-agents/.venv/lib/python3.11/site-packages/stable_baselines3/sac/sac.py:269\u001b[39m, in \u001b[36mSAC.train\u001b[39m\u001b[34m(self, gradient_steps, batch_size)\u001b[39m\n\u001b[32m    267\u001b[39m \u001b[38;5;28mself\u001b[39m.critic.optimizer.zero_grad()\n\u001b[32m    268\u001b[39m critic_loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# Compute actor loss\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# Alternative: actor_loss = th.mean(log_prob - qf1_pi)\u001b[39;00m\n\u001b[32m    273\u001b[39m \u001b[38;5;66;03m# Min over all critic networks\u001b[39;00m\n\u001b[32m    274\u001b[39m q_values_pi = th.cat(\u001b[38;5;28mself\u001b[39m.critic(replay_data.observations, actions_pi), dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RL/exercises/rl-week-3-rl-agents/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RL/exercises/rl-week-3-rl-agents/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RL/exercises/rl-week-3-rl-agents/.venv/lib/python3.11/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RL/exercises/rl-week-3-rl-agents/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RL/exercises/rl-week-3-rl-agents/.venv/lib/python3.11/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RL/exercises/rl-week-3-rl-agents/.venv/lib/python3.11/site-packages/torch/optim/adam.py:456\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    454\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    459\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "env = gym.make(env_id)\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_dir)\n",
    "model.learn(total_timesteps=500_000)\n",
    "model.save(model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c528b3-c890-4d43-9854-6af07bfcc94f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7383350",
   "metadata": {},
   "source": [
    "## Evaluate Trained and Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cea3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "\n",
    "n_eval_episodes = 10\n",
    "\n",
    "\n",
    "# Create/ load agents\n",
    "\n",
    "# Untrained agent\n",
    "env = gym.make(env_id)\n",
    "untrained_agent = SAC(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "# Trained agents\n",
    "trained_agent = SAC.load(model_fn)\n",
    "\n",
    "agents = {\n",
    "    \"untrained\": untrained_agent,\n",
    "    \"trained\": trained_agent,\n",
    "}\n",
    "\n",
    "# Create env to evaluate\n",
    "env = DummyVecEnv([lambda: gym.make(env_id)])\n",
    "env = Monitor(gym.make(env_id))\n",
    "\n",
    "# Evaluate each agent and gather results\n",
    "results = []\n",
    "for name, agent in agents.items():\n",
    "    # Rollout n_eval_episodes and record performance\n",
    "    means, stds = evaluate_policy(\n",
    "        agent, env, n_eval_episodes=n_eval_episodes, return_episode_rewards=True\n",
    "    )\n",
    "    performance = np.mean(means)\n",
    "    results.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"agent\": name,\n",
    "                \"episode\": np.arange(0, n_eval_episodes),\n",
    "                \"reward\": means,\n",
    "                \"length\": stds,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "results = pd.concat(results).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as printr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = results\n",
    "printr(df)\n",
    "\n",
    "ax = sns.boxplot(data=df, x=\"agent\", y=\"reward\")\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(data=df, x=\"agent\", y=\"length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "video_folder = \"logs/videos/\"\n",
    "video_length = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Record random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "# This creates a vectorized env (here of length 1). Is useful for parallelization\n",
    "# when we have several workers that can perform individual rollouts.\n",
    "env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "\n",
    "env = VecVideoRecorder(\n",
    "    env,\n",
    "    video_folder,\n",
    "    record_video_trigger=lambda x: x == 0,\n",
    "    video_length=video_length,\n",
    "    name_prefix=f\"random-agent-{env_id}\",\n",
    ")\n",
    "\n",
    "n_steps = video_length\n",
    "\n",
    "# Reset and initialize the environment\n",
    "obs = env.reset()\n",
    "\n",
    "# For the number of frames\n",
    "for i in range(n_steps):\n",
    "    # Sample a random action from the action space\n",
    "    action = [env.action_space.sample()]\n",
    "\n",
    "    # Step the environment\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        # If the env signals the end of the episode, stop recording\n",
    "        break\n",
    "        obs = env.reset()\n",
    "# Close / cleanup env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Record trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "\n",
    "# Record the video starting at the first step\n",
    "env = VecVideoRecorder(\n",
    "    env,\n",
    "    video_folder,\n",
    "    record_video_trigger=lambda x: x == 0,\n",
    "    video_length=video_length,\n",
    "    name_prefix=f\"trained-agent-{env_id}\",\n",
    ")\n",
    "\n",
    "model = SAC.load(model_fn)\n",
    "\n",
    "n_steps = video_length\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(n_steps):\n",
    "    # Now PREDICT the action to take with the trained model ðŸ¤–\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "        obs = env.reset()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
